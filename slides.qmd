---
title: "MLOps: Deploying Models into Production"
author: "Myles Mitchell @ Jumping Rivers"
format:
  jrSlides-revealjs:
    code-link: true
    width: 1600
    height: 900
    embed-resources: true
---

## Welcome!

:::{.columns}
:::{.column width="50%"}
* Slides: [bit.ly/2024-nicd-mlops](https://bit.ly/2024-nicd-mlops)

* GitHub: [github.com/jumpingrivers/2024-nicd-mlops](https://github.com/jumpingrivers/2024-nicd-mlops)

* Access the virtual environment:

  * URL: [https://mlops-intro.jumpingrivers.training/welcome/](https://mlops-intro.jumpingrivers.training/welcome/)

  * Master password: **pomelo-jujube**
:::
:::{.column width="50%"}
<center>
**Check in using the QR code!**

![](img/pm-check-in.jpg){width="40%" fig-alt="QR code for afternoon session check-in"}
</center>
:::
:::

## Virtual environment

::: {style="text-align: center;"}

**https://mlops-intro.jumpingrivers.training/welcome/**

Password: **pomelo-jujube**

:::

![](img/vm.png){fig-alt="Screenshot of log in page" fig-align="center"}

# Before we start...

## Who am I?

:::: {.columns}

::: {.column width="60%"}
:::{.incremental}
- Background in Astrophysics.

- Data Scientist @ Jumping Rivers:

  - Python & R support for various clients.

  - Teach courses in Python, R, SQL, Machine Learning.

- Hobbies include hiking and travelling.
:::
:::

::: {.column width="40%"}
![](img/myles.jpg)
:::

::::

## Jumping Rivers

:::{.columns}
:::{.column width="58%"}
[‚Üó jumpingrivers.com](https://www.jumpingrivers.com/) &nbsp; [ùïè @jumping_uk](https://twitter.com/jumping_uk)

- Machine learning
- Dashboard development
- R packages and APIs
- Data pipelines
- Code review

<center>
![](img/R_logo.png){width="23%"}  &nbsp; ![](img/posit.png){width="19%"} &nbsp; ![](img/stan.png){width="18%"}  &nbsp; ![](img/python.png){width="18%"}
</center>
:::
:::{.column width="42%"}
<center>
![](img/jr-logo.png)
</center>
:::
:::

## How will this workshop run?

:::{.incremental}
* Presentation slides
  ([bit.ly/2024-nicd-mlops](https://bit.ly/2024-nicd-mlops)).

* R coding demos in virtual environment.

* Exercises will apply the demo code to a different dataset and model.

* Not an R user?
  * The complete scripts and solutions are available.
  * Not all exercises will be code-based.
:::

## What will be covered?

:::{.incremental}
* Introduction to MLOps

* Building a basic MLOps workflow

* We will be deploying models **locally** only...
  * Means all tools are opensource and free!

* Want to contribute?
  * Raise your hand for questions and comments (bonus points...).
:::

## What you get from me

:::{.incremental}
* GitHub repo
  ([github.com/jumpingrivers/2024-nicd-mlops](https://github.com/jumpingrivers/2024-nicd-mlops)):
  * HTML slides (also available at [bit.ly/2024-nicd-mlops](https://bit.ly/2024-nicd-mlops)).
  * Demo and exercise scripts.

* Useful links to other resources.
:::

## Discussion

:::{.incremental}
*   What is MLOps?

*   **Discuss**

*   Extra points if you contribute...

*   QR Code:

    <center>
    ![](img/pm-extra-points.jpg)
    </center>
:::

```{r}
countdown::countdown(minutes = 5,
                    color_border = "#490f3a",
                    color_text = "#490f3a",
                    color_running_text = "#fcfbfa",
                    color_running_background = "#490f3a",
                    color_finished_text = "#490f3a",
                    color_finished_background = "#fcfbfa",
                    top = 0,
                    margin = "1.2em",
                    font_size = "2em")
```


# Introduction to MLOps

## Let's take a step back...

The typical data science workflow:

<center>
![](img/ds-flow.png){fig-alt="Typical data science workflow. Starting with data importing and tidying, followed by a cycle of data transformation, data visualisation and modelling which repeats as the model is better understood. The results from this cycle are then communicated."}
</center>

:::{.incremental}
* Data is imported and tidied.
* Cycle of data transformation, data visualisation and modelling.
* The cycle repeats as we understand the underlying model better.
* The results are communicated to an external audience.
:::

## From Classical Stats to Machine Learning

:::{.incremental}
* The classical workflow prioritises understanding the *system* behind the data.
* By contrast, Machine Learning prioritises *prediction*.
* As data grows we reconsider and update our ML models to optimise predictive
  power.
* A goal of MLOps is to streamline this cycle.
:::

## What is MLOps?

MLOps: Machine Learning Operations

<center>
![](img/mlops-flow.png){fig-alt="MLOps workflow. Starting with data importing and tidying, followed by modelling and finishing with model versioning, deployment and monitoring. The cycle then repeats as more data is acquired."}
</center>

:::{.incremental}
* Framework to continuously build, deploy and maintain ML models.
* Encapsulates the "full stack" from data acquisition to model deployment.
* Monitor models in production and detect "model drift".
* Versioning of models and data.
:::

## MLOps frameworks

* Amazon SageMaker
* Google Cloud Platform
* Kubeflow (ML toolkit for Kubernetes)
* Vetiver by Posit (free to install, nice for beginners)
* And the list goes on...

<center>
![](img/sagemaker-logo.jpg){width="19%"}  &nbsp; ![](img/azure-logo.png){width="20%"} &nbsp; ![](img/gcp-logo.png){width="20%"}  &nbsp; ![](img/vetiver-logo.png){width="17%"}

## Vetiver

:::{.columns}
:::{.column width="60%"}
:::{.incremental}
* Opensource tool maintained by Posit (formerly RStudio).
* Integrates with popular ML libraries in R and Python.
* Fluent tooling to version, deploy and monitor a trained model.
* Supports deploying models to localhost - great way to learn MLOps!
:::
:::
:::{.column width="40%"}
<center>
![](img/posit-logo.png)
![](img/vetiver-logo.png){width="60%"}
</center>
:::
:::

# Your first MLOps pipeline

## Let's build an MLOps stack!

:::{.incremental}
* Data
* Modelling
* Deployment
* Monitoring
* Repeat
:::

## Modelling penguins!

Let's set up a **basic** MLOps workflow!

:::{.incremental}
*   Palmer Penguins dataset:

    ```{r}
    #| echo: true
    #| code-line-numbers: 1,2,3|1|3
    library("palmerpenguins")

    names(penguins)
    ```
:::

---

```{r}
#| echo: false
#| message: false
#| output-location: slide
#| fig-cap: "Palmer Penguin dataset"
#| fig-alt: "Scatter plot showing positive relationship between penguin flipper length and penguin body mass. The data points are coloured based on species and shaped based on island. The Gentoo penguins tend to have higher body mass and flipper length than Adelie and Chinstrap."
library("ggplot2")

ggplot(penguins, aes(flipper_length_mm, body_mass_g)) +
  geom_point(aes(colour = species, shape = island)) +
  theme_minimal() +
  xlab("Flipper Length(mm)") +
  ylab("Body Mass(g)") +
  viridis::scale_colour_viridis(discrete = TRUE)
```

## Data tidying and cleaning

Let's predict species using flipper length, body mass and island!

:::{.incremental}
*   Using {tidyr} and {rsample}:

    ```{r}
    #| echo: true
    #| code-line-numbers: 1,2,3,4,5,6,7,8,9|1,2|4,5,6,7|8,9
    # Drop missing data
    penguins_data = tidyr::drop_na(penguins)
    
    # Split into train and test sets
    penguins_split = rsample::initial_split(
      penguins_data, prop = 0.8
    )
    train_data = rsample::training(penguins_split)
    test_data = rsample::testing(penguins_split)
    ```
:::

# Modelling penguins!

Open **demo.R** in RStudio...

## Task 1: Data loading and tidying

* Open **exercises.R**

:::{.incremental}
*   Attempt **"Task 1: Data loading and tidying"**

*   Need help? Check **demo.R** or raise your hand

*   Not an R user? The solution can be found in **solutions.R**

*   Finished? Scan the QR code for extra points!

    <center>
    ![](img/pm-extra-points.jpg)
    </center>
:::

```{r}
countdown::countdown(minutes = 10,
                    color_border = "#490f3a",
                    color_text = "#490f3a",
                    color_running_text = "#fcfbfa",
                    color_running_background = "#490f3a",
                    color_finished_text = "#490f3a",
                    color_finished_background = "#fcfbfa",
                    top = 0,
                    margin = "1.2em",
                    font_size = "2em")
```

## Data best practices

:::{.columns}
:::{.column width="60%"}
Loading and importing

:::{.incremental}
* Consider moving from large CSV files to a more efficient format like Apache
  Parquet and Apache Arrow.
* Add a data validation check to prevent unexpected data formats entering the
  pipeline.
* Consider tools like Apache Spark for fast processing of Big Data.
:::
:::

:::{.column width="40%"}
<center>
![](img/parquet-logo.png)
</center>
:::
:::

## Data best practices

:::{.columns}
:::{.column width=60%}
Tidying & cleaning

:::{.incremental}
* Consider creating an R package to encourage proper documentation, testing and dependency management.
* Optimise bottlenecks to improve efficiency.
* Split into training and validation sets.
:::
:::

:::{.column width="40%"}
<center>
![](img/tidyverse-logo.png)
</center>
:::
:::

## Data best practices

:::{.columns}
:::{.column width="60%"}
Versioning

:::{.incremental}
* Ensuring reproducibility is vital
  * Example: why did your model have poor accuracy for ethic minority background individuals?
* Include timestamps in your database queries.
* Ensure your training set can be retrieved in the future.
:::
:::

:::{.column width="40%"}
<center>
![](img/git-logo.png)
![](img/pins-logo.png)
</center>
:::
:::

## Data best practices

:::{.columns}
:::{.column width="60%"}
Take advantage of native tools on ML platforms

:::{.incremental}
* Your preferred ML platform probably has built-in tools for data wrangling!
  * SageMaker Data Wrangler
  * H2O.ai feature engineering (no code)
:::
:::

:::{.column width="40%"}
<center>
![](img/h2o-logo.png)
</center>
:::
:::

## Modelling penguins!

* Let's set up the model recipe in {tidymodels}:

```{r}
#| echo: true
#| code-line-numbers: 1,2,3,4,5,6,7,8|1|3,4,5,6|7|8
library("tidymodels")

model = recipe(
  species ~ island + flipper_length_mm + body_mass_g,
  data = train_data
) |>
  workflow(nearest_neighbor(mode = "classification")) |>
  fit(train_data)
```

## Modelling penguins!

Our model object can now be used to predict `species`{.r}:

```{r}
#| echo: true
#| code-line-numbers: 1,2,3,4,5,6,7,8|1|3,4,5,6,7,8
model_pred = predict(model, test_data)

# Accuracy for unseen test data
mean(
  model_pred$.pred_class == as.character(
    test_data$species
  )
)
```

## Enter Vetiver!

:::{.incremental}
*   Use a *Vetiver model* object to collate all of the info needed to store,
    deploy and version our model:

    ```{r}
    #| echo: true
    #| code-line-numbers: 1,2,3,4,5,6|1,2,3,4,5|2|3|4|6
    v_model = vetiver::vetiver_model(
      model,
      model_name = "k-nn",
      description = "penguin-species"
    )
    v_model
    ```
:::

## Vetiver model

`v_model`{.r} is a list with six elements

:::{.columns}
:::{.column width="50%"}

:::{.incremental}
*   View the contents:

    ```{r}
    #| echo: true
    names(v_model)
    ```

*   View the model description:

    ```{r}
    #| echo: true
    v_model$description
    ```
:::
:::

:::{.column width="50%"}
:::{.incremental}
*   View the metadata:

    ```{r}
    #| echo: true
    v_model$metadata
    ```
:::
:::
:::

# Demo

## Task 2: Modelling

* Open **exercises.R** (and rerun your Task 1 code)

:::{.incremental}
*   Attempt **"Task 2: Modelling"**

*   Need help? Check **demo.R** or raise your hand

*   Not an R user? The solution can be found in **solutions.R**

*   Finished? Scan the QR code for extra points!

    <center>
    ![](img/pm-extra-points.jpg)
    </center>
:::

```{r}
countdown::countdown(minutes = 10,
                    color_border = "#490f3a",
                    color_text = "#490f3a",
                    color_running_text = "#fcfbfa",
                    color_running_background = "#490f3a",
                    color_finished_text = "#490f3a",
                    color_finished_background = "#fcfbfa",
                    top = 0,
                    margin = "1.2em",
                    font_size = "2em")
```

## Modelling best practices

:::{.columns}
:::{.column width="60%"}
Choosing the right model can be tough!

:::{.incremental}
* Use cheatsheets like [datacamp.com/cheat-sheet/machine-learning-cheat-sheet](https://www.datacamp.com/cheat-sheet/machine-learning-cheat-sheet) to identify potential model families.
* Include cross validation to optimise hyperparameters.
* Try auto-ML tools like H2O.ai and SageMaker Autopilot.
:::
:::

:::{.column width="40%"}
<center>
![](img/h2o-logo.png)
![](img/sagemaker-logo.jpg)
</center>
:::
:::

## Modelling best practices

:::{.columns}
:::{.column width="60%"}
Versioning

:::{.incremental}
* Store scoring metrics and model parameters from each experiment.
* Any previously-deployed model should be retrievable, along with the data used
  to train it.
:::
:::

:::{.column width="40%"}
<center>
![](img/git-logo.png)
![](img/pins-logo.png)
</center>
:::
:::

## Model versioning

:::{.incremental}
*   Use {pins} to store R or Python objects for reuse later.

*   Store pins using "boards" including Posit Connect, Amazon S3 or even Google
    drive!

*   Storing in a temporary directory:

    ```{r}
    #| echo: true
    #| code-line-numbers: 1,2|1|2
    model_board = pins::board_temp(versioned = TRUE)
    model_board |> vetiver::vetiver_pin_write(v_model)
    ```
:::

## Retrieving a pinned model

:::{.incremental}
*   Retrieve a model

    ```{r}
    #| echo: true
    model_board |> vetiver::vetiver_pin_read("k-nn")
    ```

*   Inspect the stored versions

    ```{r}
    #| echo: true
    model_board |> pins::pin_versions("k-nn")
    ```
:::

# Model deployment

## Deployment using Vetiver

:::{.columns}
:::{.column width="60%"}
:::{.incremental}
*   We deploy models as APIs which take input data and send back model predictions.

*   We can use a {plumber} API to deploy a {vetiver} model.
:::
:::

:::{.column width="40%"}
<center>
![](img/plumber-logo.png)
</center>
:::
:::

## Deploying locally

:::{.incremental}
*   {vetiver} and {plumber} support local deployment:

    ```{r}
    #| echo: true
    #| eval: false
    #| code-line-numbers: 1,2,3|1|2|3
    plumber::pr() |>
      vetiver::vetiver_api(v_model) |>
      plumber::pr_run()
    ```

*   Opens the API in a browser window

*   Great for beginners to MLOps and APIs!
:::

## Deploying locally

Check the deployment with:

```{r}
#| echo: true
#| eval: false
#| code-line-numbers: 1,2,3,4,5,6,7|1|2,3|4,5,6,7
base_url = "127.0.0.1:8080/"  # update the port number!
url = paste0(base_url, "ping")
r = httr::GET(url)
metadata = httr::content(
  r, as = "text", encoding = "UTF-8"
)
jsonlite::fromJSON(metadata)
```

## Model predictions

Checking that our API works!

:::{.incremental}
*   Endpoints `metadata` and `predict` allow programmatic queries:

    ```{r}
    #| echo: true
    #| eval: false
    #| code-line-numbers: 1,2,3,4,5,6,7,8|1|2|3,4,5,6,7|8
    url = paste0(base_url, "predict")
    endpoint = vetiver::vetiver_endpoint(url)
    pred_data = test_data |>
      dplyr::select(
        "island", "flipper_length_mm", "body_mass_g"
      ) |>
      dplyr::slice_sample(n = 10)
    predict(endpoint, pred_data)
    ```
:::

# Demo

## Task 3: Deploying your model

* Open **exercises.R**

:::{.incremental}
*   Attempt **"Task 3: Deploying your model"**

*   Need help? Check **demo.R** or raise your hand

*   Not an R user? The solution can be found in **solutions.R**

*   Finished? Scan the QR code for extra points!

    <center>
    ![](img/pm-extra-points.jpg)
    </center>
:::

```{r}
countdown::countdown(minutes = 10,
                    color_border = "#490f3a",
                    color_text = "#490f3a",
                    color_running_text = "#fcfbfa",
                    color_running_background = "#490f3a",
                    color_finished_text = "#490f3a",
                    color_finished_background = "#fcfbfa",
                    top = 0,
                    margin = "1.2em",
                    font_size = "2em")
```

## Aside: What about Python?

:::{.incremental}
* Vetiver is available for both Python and R!

* In Python you would use Python ML libraries rather than {tidymodels}
  * scikit learn
  * PyTorch
  * XGBoost
  * statsmodels

* Vetiver documentation: [vetiver.posit.co](https://vetiver.posit.co/)
:::

## Deployment best practices

:::{.columns}
:::{.column width="60%"}
:::{.incremental}
* Try deploying locally to check that your model API works as expected.

* Use environment managers like {renv} to store model dependencies.

* Use containers like Docker to bundle model source code with dependencies.
:::
:::

:::{.column width="40%"}
<center>
![](img/renv-logo.png)
![](img/docker-logo.png)
</center>
:::
:::

## Deploying to the cloud

:::{.incremental}
*   Vetiver also streamlines deployment to the production environment:

    ```{r}
    #| echo: true
    #| eval: false
    #| code-line-numbers: 1,2,3,4,5|2|3|4
    vetiver::vetiver_prepare_docker(
      pins::board_connect(), 
      "myles/k-nn", 
      docker_args = list(port = 8080)
    )
    ```

*   This command:

    * Lists R depedencies with {renv}

    * Stores the {plumber} API code in `plumber.R`

    * Generates a **Dockerfile**
:::

## Docker files

:::{.columns}
:::{.column width="60%"}
Our Dockerfile contains a series of commands to:

:::{.incremental}
* Set the R version and install the system libraries.

* Install the required R packages.

* Run the API in the deployed environment.
:::
:::

:::{.column width="40%"}
<center>
![](img/docker-logo.png)
</center>
:::
:::

## Running Docker

:::{.incremental}
*   Build a Docker container:

    ```bash
    docker build --tag my-first-model .
    ```

*   Inspect your stored Docker images:

    ```bash
    docker image list
    ```

*   Run the image:

    ```r
    docker run --rm --publish 8080:8080 my-first-model
    ```

*   These steps can be run in sequence using a CI/CD pipeline.
:::

## Deploying to Connect

:::{.incremental}
*   Vetiver integrates nicely with Posit Connect:

    ```r
    vetiver::vetiver_deploy_rsconnect(
      board = pins::board_connect(), "myles/k-nn"
    )
    ```

*   We can also publish to Amazon SageMaker using
    `vetiver_deploy_sagemaker()`{.r}
:::

## Cost considerations for cloud MLOps

Cloud MLOps normally doesn't come free...

:::{.incremental}
* Some platforms offer free trials (e.g., SageMaker).

* May be cheaper if you're already invested in a particular cloud platform
  * Data services
  * App deployment

* Costs can rise depending on computational resources consumed.

* Model building and deployment use different environments.
:::

# Monitoring your model

Deployment is just the beginning...

## Discussion - Model monitoring

:::{.incremental}
*   Why should a deployed model be closely monitored?

*   What warning signs would you look out for?

*   **Discuss**

*   Bonus points if you contribute...

*   QR Code:

    <center>
    ![](img/pm-extra-points.jpg)
    </center>
:::

```{r}
countdown::countdown(minutes = 5,
                    color_border = "#490f3a",
                    color_text = "#490f3a",
                    color_running_text = "#fcfbfa",
                    color_running_background = "#490f3a",
                    color_finished_text = "#490f3a",
                    color_finished_background = "#fcfbfa",
                    top = 0,
                    margin = "1.2em",
                    font_size = "2em")
```

## Model drift

:::{.incremental}
* Model performance may *drift* as the data evolves...
  * *Data* drift: statistical distribution of input feature changes.
  * *Concept* drift: relationship between target and input variables changes.

* The context in which your model was trained matters!
:::

<center>
![](img/polar-bears.jpg) 
</center>

## Task 4: Detecting model drift

* Open **exercises.R**

:::{.incremental}
*   Attempt **"Task 4: Detecting model drift"**

*   Need help? Check **demo.R** or raise your hand

*   Not an R user? The solution can be found in **solutions.R**

*   Finished? Scan the QR code for extra points!

    <center>
    ![](img/pm-extra-points.jpg)
    </center>
:::

```{r}
countdown::countdown(minutes = 10,
                    color_border = "#490f3a",
                    color_text = "#490f3a",
                    color_running_text = "#fcfbfa",
                    color_running_background = "#490f3a",
                    color_finished_text = "#490f3a",
                    color_finished_background = "#fcfbfa",
                    top = 0,
                    margin = "1.2em",
                    font_size = "2em")
```

## Model monitoring

:::{.incremental}
* As your data grows, run regular checks of model performance.

* Monitor key model metrics over time.

* You may notice a downward trend...

* Retrain the model with the latest data and redeploy.
:::

## Monitoring demand

As data and user base grows, your model needs to *scale*.

:::{.incremental}
* Upgrade your computational resources.

* Consider moving from a relational database to a data warehouse.

* Check how many users your license (AWS, Posit, etc) permits.
:::

## Monitoring with Vetiver

Vetiver has built in functions to track scoring metrics over time.

:::{.incremental}
* Requires a time variable in the dataset.

* Load the model from your {pins} board.

* Make sure you are scoring the *deployed* version.

* Specify the *period* for scoring (weeks, months, years, ...).

* Model metrics can also be stored with {pins}!
:::

## Monitoring with Vetiver

Consider our life expectancy data from the exercises...

:::{.incremental}
*   Compute scoring metrics over specified period:

    ```{r}
    #| echo: true
    #| eval: false
    #| code-line-numbers: 1,2,3,4,5,6|1,2,3|4,5,6
    new_metrics = vetiver::augment(
      v_model, new_data = recent_data
    ) |>
      vetiver::vetiver_compute_metrics(
        Date, "year", `Life expectancy`, .pred
      )
    ```

*   Requires a `Date` column (generate from `Year`).

*   `recent_data`: could be data from the past year or *all* historical data.
:::

## Monitoring with Vetiver

*   Pin the metrics

    ```{r}
    #| echo: true
    #| eval: false
    #| code-line-numbers: 1,2,3,4|1|2,3,4|3
    model_board |>
      vetiver::vetiver_pin_metrics(
        new_metrics, "k-nn_metrics", overwrite = TRUE
      )
    ```

:::{.incremental}
*   Plot the metrics

    ```{r}
    #| echo: true
    #| eval: false
    #| code-line-numbers: 1,2,3,4,5,6|3,4|5,6
    library("ggplot2")
    
    monitoring_metrics = model_board |>
      pins::pin_read("k-nn_metrics")
    vetiver::vetiver_plot_metrics(monitoring_metrics) +
      scale_size(range = c(2, 4))
    ```
:::

# Closing thoughts

## Pros / cons of MLOps

:::{.incremental}
* Retraining and redeployment can happen at the click of a button.

* Encourages good practices like model versioning and packaging of source code.

* Reduces human error.

* Well defined and reproducible.

* **Consider whether it is worth the cost/effort before starting.**
:::

## Thanks for listening!

:::{.columns}
:::{.column width="50%}
:::{.incremental}
* **Remember to provide feedback!**

* Open this workshop in the conference app

* Tap "More"

* Tap "Feedback"

* Fill in the form
:::
:::

:::{.column width="50%"}
<center>
**Workshop QR code**

![](img/pm-check-in.jpg){width="40%" fig-alt="QR code for afternoon session check-in"}
</center>
:::
:::

## Links & Resources

:::{style="font-size: 45px;"}
:::{.columns}
:::{.column width="45%"}
* Slides: [bit.ly/2024-nicd-mlops](https://bit.ly/2024-nicd-mlops)
* GitHub repo: [github.com/jumpingrivers/2024-nicd-mlops](https://github.com/jumpingrivers/2024-nicd-mlops)
* Jumping Rivers (courses & blog): [jumpingrivers.com](https://www.jumpingrivers.com/)
* Vetiver docs: [vetiver.posit.co](https://vetiver.posit.co/)
* MLOps platforms compared: [https://valohai.com/mlops-platforms-compared/](valohai.com/mlops-platforms-compared/)
:::
:::{.column width="55%"}
<center>
![](img/sip-logo.png)
</center>

* Join us for Shiny In Production (9-10 Oct)
  * [shiny-in-production.jumpingrivers.com](https://shiny-in-production.jumpingrivers.com/)
  * Discount code: **JRSIPNICD**
:::
:::
:::
